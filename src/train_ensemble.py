%%writefile /content/face_system/src/train_ensemble.py
import os
import json
import pickle
import numpy as np
import cv2
from tqdm import tqdm
from numpy.linalg import norm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from insightface.app import FaceAnalysis


# ================================
# è¨­å®š
# ================================
DATA_DIR = "/content/drive/MyDrive/face_DataSet"
RAW_DIR = f"{DATA_DIR}/face_raw"
MODEL_DIR = "/content/face_system/models"

os.makedirs(MODEL_DIR, exist_ok=True)


def imread_safe(path):
    """æ”¯æ´ä¸­æ–‡/ç©ºæ ¼è·¯å¾‘"""
    return cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)


# ================================
# åµæ¸¬æ–°å¢äººç‰©
# ================================
def load_old_data():
    X_path = os.path.join(MODEL_DIR, "X.npy")
    y_path = os.path.join(MODEL_DIR, "y.npy")

    if os.path.exists(X_path) and os.path.exists(y_path):
        print("ğŸ“‚ è¼‰å…¥èˆŠè³‡æ–™ X.npy / y.npy")
        return np.load(X_path), np.load(y_path)
    print("âš ï¸ ç„¡èˆŠè³‡æ–™ï¼Œç¬¬ä¸€æ¬¡è¨“ç·´")
    return np.array([]), np.array([])


def detect_new_persons(y_old):
    """æª¢æŸ¥ RAW_DIR è£¡å“ªäº›è³‡æ–™å¤¾æ²’è¨“ç·´é"""
    persons = sorted(os.listdir(RAW_DIR))
    old_people = set(y_old.tolist()) if len(y_old) > 0 else set()

    new_list = [p for p in persons if p not in old_people]
    print(f"\nğŸ†• æ–°å¢äººç‰©ï¼š{new_list}")
    return new_list


# ================================
# å»ºç«‹ embeddings
# ================================
app = FaceAnalysis(name="buffalo_l")
app.prepare(ctx_id=0)


def extract_embeddings(person_list):
    X, y = [], []

    for person in person_list:
        p_dir = os.path.join(RAW_DIR, person)
        images = os.listdir(p_dir)

        print(f"\nğŸ“¸ {person} â€” {len(images)} å¼µåœ–ç‰‡")

        for img_name in tqdm(images):
            img_path = os.path.join(p_dir, img_name)
            img = imread_safe(img_path)
            if img is None:
                continue

            faces = app.get(img)
            if len(faces) == 0:
                continue

            emb = faces[0].normed_embedding
            X.append(emb)
            y.append(person)

    return np.array(X), np.array(y)


# ================================
# 3 åˆ†é¡å™¨è¨“ç·´
# ================================
def train_knn(X, y):
    print("\nğŸš€ è¨“ç·´ KNN ...")
    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X, y)
    return knn


def train_svm(X, y):
    print("\nğŸš€ è¨“ç·´ SVM ...")
    svm = SVC(kernel="linear", probability=True)
    svm.fit(X, y)
    return svm


def calc_centers(X, y):
    print("\nğŸš€ è¨ˆç®— Centers ...")
    centers = {}
    labels = np.unique(y)
    for person in labels:
        centers[person] = X[y == person].mean(axis=0)
    return centers


# ================================
# é–€æª»è‡ªå‹•å¾®èª¿ï¼ˆUnknown æœ€é‡è¦çš„éƒ¨åˆ†ï¼‰
# ================================
def compute_cosine(a, b):
    return np.dot(a, b) / (norm(a) * norm(b))


def auto_threshold(X, y, centers):
    print("\nğŸ§  è‡ªå‹•å¾®èª¿ Unknown é–€æª» ...")

    pos = []
    neg = []

    for emb, label in zip(X, y):
        # æ­£ä¾‹ similarity
        pos.append(compute_cosine(emb, centers[label]))

        # è² ä¾‹ similarity
        for other, vec in centers.items():
            if other != label:
                neg.append(compute_cosine(emb, vec))

    pos = np.array(pos)
    neg = np.array(neg)

    # é–€æª»å»ºè­°ä½œæ³•ï¼šè² ä¾‹çš„ Î¼ + 1.5Ïƒ
    thr = neg.mean() + 1.5 * neg.std()

    thr = float(max(min(thr, 0.60), 0.30))  # å®‰å…¨é™åˆ¶å€é–“
    print(f"ğŸ“Œ å»ºè­°é–€æª»ï¼š{thr:.4f}")

    return thr


# ================================
# å„²å­˜
# ================================
def save_all(X, y, knn, svm, centers, label_map, threshold):
    np.save(f"{MODEL_DIR}/X.npy", X)
    np.save(f"{MODEL_DIR}/y.npy", y)

    with open(f"{MODEL_DIR}/knn.pkl", "wb") as f:
        pickle.dump(knn, f)

    with open(f"{MODEL_DIR}/svm.pkl", "wb") as f:
        pickle.dump(svm, f)

    with open(f"{MODEL_DIR}/centers.pkl", "wb") as f:
        pickle.dump(centers, f)

    with open(f"{MODEL_DIR}/label_map.json", "w") as f:
        json.dump(label_map, f, ensure_ascii=False, indent=2)

    with open(f"{MODEL_DIR}/threshold.json", "w") as f:
        json.dump({"cosine_threshold": threshold}, f, indent=2)

    print("\nğŸ’¾ æ‰€æœ‰æ¨¡å‹/è³‡æ–™å·²ä¿å­˜å®Œç•¢ï¼")


# ================================
# Main
# ================================
if __name__ == "__main__":
    X_old, y_old = load_old_data()
    new_list = detect_new_persons(y_old)

    if len(new_list) == 0 and len(X_old) > 0:
        print("\nâœ” æ²’æœ‰æ–°å¢äººå“¡ï¼Œä¸éœ€è¦é‡æ–°è¨“ç·´")
        exit()

    X_new, y_new = extract_embeddings(new_list)

    # åˆä½µè³‡æ–™
    X = np.concatenate([X_old, X_new]) if len(X_old) > 0 else X_new
    y = np.concatenate([y_old, y_new]) if len(y_old) > 0 else y_new

    # label_map
    label_map = {label: i for i, label in enumerate(sorted(np.unique(y)))}

    # è¨“ç·´ä¸‰åˆ†é¡å™¨
    knn = train_knn(X, y)
    svm = train_svm(X, y)
    centers = calc_centers(X, y)

    # å‹•æ…‹ Unknown é–€æª»
    thr = auto_threshold(X, y, centers)

    # å„²å­˜å…¨éƒ¨æ¨¡å‹
    save_all(X, y, knn, svm, centers, label_map, thr)

    print("\nğŸ‰ ä¸‰åˆ†é¡å™¨è¨“ç·´å®Œæˆï¼")
