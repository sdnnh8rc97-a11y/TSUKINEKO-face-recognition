import os
import json
import pickle
import numpy as np
import cv2
from tqdm import tqdm
from numpy.linalg import norm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from insightface.app import FaceAnalysis

# ================================
# è¨­å®š
# ================================
DATA_DIR = "/content/drive/MyDrive/face_DataSet"
RAW_DIR = f"{DATA_DIR}/face_raw"
MODEL_DIR = "/content/face_system/models"

os.makedirs(MODEL_DIR, exist_ok=True)

def imread_safe(path):
    return cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)

# ================================
# 1. è¼‰å…¥èˆŠè³‡æ–™ï¼ˆX, y_raw, label_mapï¼‰
# ================================
def load_old_data():
    X_path = os.path.join(MODEL_DIR, "X.npy")
    y_path = os.path.join(MODEL_DIR, "y.npy")
    map_path = os.path.join(MODEL_DIR, "label_map.json")

    if os.path.exists(X_path) and os.path.exists(y_path) and os.path.exists(map_path):
        print("ğŸ“‚ è¼‰å…¥èˆŠè³‡æ–™ X.npy / y.npy / label_map.json")

        X = np.load(X_path)
        y_index = np.load(y_path)
        label_map = json.load(open(map_path, "r", encoding="utf-8"))

        inv_map = {int(v): k for k, v in label_map.items()}
        y_raw = np.array([inv_map[idx] for idx in y_index])

        return X, y_raw, label_map

    print("âš ï¸ ç¬¬ä¸€æ¬¡è¨“ç·´ï¼Œæœªæ‰¾åˆ°èˆŠæ¨¡å‹")
    return np.array([]), np.array([]), {}

# ================================
# 2. åµæ¸¬æ–°å¢ / åˆªé™¤äººå“¡
# ================================
def detect_person_change(label_map_old):
    old_names = set(label_map_old.keys())
    current_names = set(os.listdir(RAW_DIR))

    deleted = old_names - current_names
    added = current_names - old_names

    changed = False

    if deleted:
        print(f"âš ï¸ åµæ¸¬åˆ°äººç‰©è¢«åˆªé™¤ï¼š{deleted}")
        changed = True
    if added:
        print(f"ğŸ†• åµæ¸¬åˆ°æ–°å¢äººç‰©ï¼š{added}")
        changed = True

    return changed

# ================================
# 3. åµæ¸¬ç…§ç‰‡æ•¸é‡è®Šå‹•
# ================================
def detect_image_count_changed():
    record_path = os.path.join(MODEL_DIR, "image_count.json")

    if not os.path.exists(record_path):
        return True

    old_record = json.load(open(record_path, "r", encoding="utf-8"))
    new_record = {}
    changed = False

    for person in os.listdir(RAW_DIR):
        p_dir = os.path.join(RAW_DIR, person)
        if not os.path.isdir(p_dir):
            continue

        count = len(os.listdir(p_dir))
        new_record[person] = count

        if person not in old_record or old_record[person] != count:
            print(f"âš ï¸ {person} çš„ç…§ç‰‡æ•¸é‡æ”¹è®Šï¼Œéœ€è¦é‡æ–°è¨“ç·´")
            changed = True

    json.dump(new_record, open(record_path, "w", encoding="utf-8"), indent=2)
    return changed

# ================================
# 4. æå– embeddings
# ================================
app = FaceAnalysis(name="buffalo_l")
app.prepare(ctx_id=0)

def extract_embeddings(person_list):
    X, y = [], []
    for person in person_list:
        p_dir = os.path.join(RAW_DIR, person)
        imgs = os.listdir(p_dir)

        print(f"\nğŸ“¸ {person} â€” {len(imgs)} å¼µåœ–ç‰‡")

        for img in tqdm(imgs):
            path = os.path.join(p_dir, img)
            img = imread_safe(path)
            if img is None:
                continue

            faces = app.get(img)
            if len(faces) == 0:
                continue

            X.append(faces[0].normed_embedding)
            y.append(person)

    return np.array(X), np.array(y)

# ================================
# 5. ä¸‰åˆ†é¡å™¨
# ================================
def train_knn(X, y):
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X, y)
    return model

def train_svm(X, y):
    model = SVC(kernel="linear", probability=True)
    model.fit(X, y)
    return model

def calc_centers(X, y):
    centers = {}
    for person in np.unique(y):
        centers[person] = X[y == person].mean(axis=0)
    return centers

# ================================
# 6. è‡ªå‹• thresholdï¼ˆè·é›¢ç‰ˆï¼‰
# ================================
def auto_threshold_distance(X, y):
    same_dists = []
    diff_dists = []

    for i in range(len(X)):
        for j in range(i + 1, len(X)):
            d = np.linalg.norm(X[i] - X[j])
            if y[i] == y[j]:
                same_dists.append(d)
            else:
                diff_dists.append(d)

    same_dists = np.array(same_dists)
    diff_dists = np.array(diff_dists)

    print(f"âœ” SAME avgï¼š{same_dists.mean():.4f}")
    print(f"âŒ DIFF avgï¼š{diff_dists.mean():.4f}")

    # Youdenâ€™s J
    candidates = np.linspace(0, 2, 2000)
    best_j = -1
    best_t = 0

    for t in candidates:
        tp = np.sum(same_dists <= t)
        fn = np.sum(same_dists > t)
        tn = np.sum(diff_dists > t)
        fp = np.sum(diff_dists <= t)

        sens = tp / (tp + fn + 1e-6)
        spec = tn / (tn + fp + 1e-6)
        J = sens + spec - 1

        if J > best_j:
            best_j = J
            best_t = t

    thresholds = {
        "conservative": float(same_dists.max() + 0.02),
        "balanced": float(best_t),
        "loose": float(diff_dists.min() - 0.02)
    }

    json.dump(thresholds, open(os.path.join(MODEL_DIR, "threshold.json"), "w"), indent=2)
    return thresholds

# ================================
# 7. å„²å­˜
# ================================
def save_all(X_raw, y_index, knn, svm, centers, label_map, thresholds):
    np.save(f"{MODEL_DIR}/X.npy", X_raw)
    np.save(f"{MODEL_DIR}/y.npy", y_index)

    pickle.dump(knn, open(f"{MODEL_DIR}/knn.pkl", "wb"))
    pickle.dump(svm, open(f"{MODEL_DIR}/svm.pkl", "wb"))
    pickle.dump(centers, open(f"{MODEL_DIR}/centers.pkl", "wb"))

    json.dump(label_map, open(f"{MODEL_DIR}/label_map.json", "w", encoding="utf-8"), ensure_ascii=False, indent=2)

    print("\nğŸ’¾ æ¨¡å‹èˆ‡è³‡æ–™ä¿å­˜å®Œæˆï¼")

# ================================
# 8. Main
# ================================
if __name__ == "__main__":
    X_old, y_old, label_map_old = load_old_data()

    must_retrain = False

    # äººåè®Šå‹•
    if detect_person_change(label_map_old):
        must_retrain = True

    # ç…§ç‰‡æ•¸è®Šå‹•
    if detect_image_count_changed():
        must_retrain = True

    if not must_retrain and len(X_old) > 0:
        print("\nâœ”ï¸ æ²’æœ‰ä»»ä½•è®ŠåŒ–ï¼Œä¸éœ€è¦é‡æ–°è¨“ç·´")
        exit()

    # --- é–‹å§‹è¨“ç·´ ---
    persons = sorted(os.listdir(RAW_DIR))
    X_new, y_new = extract_embeddings(persons)

    # æ–° label_map
    unique_names = sorted(set(y_new.tolist()))
    label_map = {name: idx for idx, name in enumerate(unique_names)}
    y_index = np.array([label_map[name] for name in y_new])

    knn = train_knn(X_new, y_new)
    svm = train_svm(X_new, y_new)
    centers = calc_centers(X_new, y_new)

    thresholds = auto_threshold_distance(X_new, y_new)

    save_all(X_new, y_index, knn, svm, centers, label_map, thresholds)

    print("\nğŸ‰ ä¸‰åˆ†é¡å™¨è¨“ç·´å®Œæˆï¼")
