import os
import json
import pickle
import numpy as np
import cv2
from tqdm import tqdm
from numpy.linalg import norm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from insightface.app import FaceAnalysis

# ================================
# è¨­å®š
# ================================
DATA_DIR = "/content/drive/MyDrive/face_DataSet"
RAW_DIR = f"{DATA_DIR}/face_raw"
MODEL_DIR = "/content/face_system/models"

os.makedirs(MODEL_DIR, exist_ok=True)


def imread_safe(path):
    """æ”¯æ´ä¸­æ–‡/ç©ºæ ¼è·¯å¾‘"""
    return cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)


# ================================
# è¼‰å…¥èˆŠè³‡æ–™
# ================================
def load_old_data():
    X_path = os.path.join(MODEL_DIR, "X.npy")
    y_path = os.path.join(MODEL_DIR, "y.npy")

    if os.path.exists(X_path) and os.path.exists(y_path):
        print("ğŸ“‚ è¼‰å…¥èˆŠè³‡æ–™ X.npy / y.npy")
        return np.load(X_path), np.load(y_path)
    print("âš ï¸ ç„¡èˆŠè³‡æ–™ï¼Œç¬¬ä¸€æ¬¡è¨“ç·´")
    return np.array([]), np.array([])


def load_old_data():
    X_path = os.path.join(MODEL_DIR, "X.npy")
    y_path = os.path.join(MODEL_DIR, "y.npy")
    map_path = os.path.join(MODEL_DIR, "label_map.json")

    if os.path.exists(X_path) and os.path.exists(y_path) and os.path.exists(map_path):
        print("ğŸ“‚ è¼‰å…¥èˆŠè³‡æ–™ X.npy / y.npy / label_map.json")

        X = np.load(X_path)
        y_index = np.load(y_path)
        label_map = json.load(open(map_path, "r", encoding="utf-8"))

        # åæŸ¥ index â†’ ä¸­æ–‡åå­—
        inv_map = {v: k for k, v in label_map.items()}
        y_raw = np.array([inv_map[str(idx)] for idx in y_index])

        return X, y_raw, label_map

    print("âš ï¸ ç¬¬ä¸€æ¬¡è¨“ç·´ï¼Œæœªæ‰¾åˆ°èˆŠæ¨¡å‹")
    return np.array([]), np.array([]), {}


# ================================
# å»ºç«‹ embeddings
# ================================
app = FaceAnalysis(name="buffalo_l")
app.prepare(ctx_id=0)

def extract_embeddings(person_list):
    X, y = [], []

    for person in person_list:
        p_dir = os.path.join(RAW_DIR, person)
        images = os.listdir(p_dir)

        print(f"\nğŸ“¸ {person} â€” {len(images)} å¼µåœ–ç‰‡")

        for img_name in tqdm(images):
            img_path = os.path.join(p_dir, img_name)
            img = imread_safe(img_path)
            if img is None:
                continue

            faces = app.get(img)
            if len(faces) == 0:
                continue

            emb = faces[0].normed_embedding
            X.append(emb)
            y.append(person)

    return np.array(X), np.array(y)


# ================================
# ä¸‰åˆ†é¡å™¨
# ================================
def train_knn(X, y):
    print("\nğŸš€ è¨“ç·´ KNN ...")
    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X, y)
    return model

def train_svm(X, y):
    print("\nğŸš€ è¨“ç·´ SVM ...")
    model = SVC(kernel="linear", probability=True)
    model.fit(X, y)
    return model

def calc_centers(X, y):
    print("\nğŸš€ è¨ˆç®— centers ...")
    centers = {}
    labels = np.unique(y)
    for person in labels:
        centers[person] = X[y == person].mean(axis=0)
    return centers


# ================================
# â­â­â­ è‡ªå‹• thresholdï¼ˆè·é›¢ç‰ˆï¼‰â­â­â­
# ================================
def auto_threshold_distance(X, y):
    print("\nğŸ“Š æ­£åœ¨è¼‰å…¥ embedding X, y ...")
    same_dists = []
    diff_dists = []

    print("ğŸ“ è¨ˆç®— SAME / DIFF è·é›¢ä¸­...\n")

    # å…¨éƒ¨ pairwise distance
    for i in range(len(X)):
        for j in range(i + 1, len(X)):
            d = np.linalg.norm(X[i] - X[j])  # L2 distance

            if y[i] == y[j]:
                same_dists.append(d)
            else:
                diff_dists.append(d)

    same_dists = np.array(same_dists)
    diff_dists = np.array(diff_dists)

    print(f"âœ” SAMEï¼ˆåŒä¸€äººï¼‰è·é›¢")
    print(f"   å¹³å‡ï¼š{same_dists.mean():.4f}")
    print(f"   æœ€å°ï¼š{same_dists.min():.4f}")
    print(f"   æœ€å¤§ï¼š{same_dists.max():.4f}\n")

    print(f"âŒ DIFFï¼ˆä¸åŒäººï¼‰è·é›¢")
    print(f"   å¹³å‡ï¼š{diff_dists.mean():.4f}")
    print(f"   æœ€å°ï¼š{diff_dists.min():.4f}")
    print(f"   æœ€å¤§ï¼š{diff_dists.max():.4f}\n")

    # ======== åµæ¸¬åš´é‡éŒ¯èª¤ï¼ˆä¸åŒäººè·é›¢ = 0ï¼‰========
    print("ğŸ•µï¸â€â™‚ï¸ æª¢æŸ¥æ˜¯å¦æœ‰ DIFF è·é›¢ = 0 ...")
    zero_dist_indices = np.where(diff_dists == 0)[0]
    if len(zero_dist_indices) > 0:
        print("â— æ³¨æ„ï¼šæœ‰ä¸åŒäººçš„ embedding å®Œå…¨ç›¸åŒï¼")
        print("   â¤ ä»£è¡¨ç…§ç‰‡è³‡æ–™éŒ¯æ”¾ or embedding éŒ¯æ··")
    else:
        print("âœ” æœªç™¼ç¾è·é›¢=0 çš„ç•°å¸¸ embedding")

    # ======== Youdenâ€™s J æœ€ä½³ threshold ========
    print("\nğŸ” æ­£åœ¨ä½¿ç”¨ Youdenâ€™s J æ‰¾æœ€ä½³ threshold...\n")

    candidates = np.linspace(0.0, 2.0, 2000)
    best_j = -1
    best_t = 0

    for t in candidates:
        tp = np.sum(same_dists <= t)
        fn = np.sum(same_dists > t)
        tn = np.sum(diff_dists > t)
        fp = np.sum(diff_dists <= t)

        sensitivity = tp / (tp + fn + 1e-6)
        specificity = tn / (tn + fp + 1e-6)

        J = sensitivity + specificity - 1

        if J > best_j:
            best_j = J
            best_t = t

    # ä¸‰ç¨®ç­–ç•¥
    t_conservative = same_dists.max() + 0.02
    t_balanced = best_t
    t_loose = diff_dists.min() - 0.02

    print("ğŸ¯ è‡ªå‹• threshold è¨ˆç®—çµæœï¼š\n")
    print(f"ğŸ”’ ä¿å®ˆï¼ˆä¸éŒ¯èªï¼‰ï¼š{t_conservative:.4f}")
    print(f"âš–ï¸ å¹³è¡¡ï¼ˆæœ€ä½³ Jï¼‰ï¼š{t_balanced:.4f}")
    print(f"ğŸˆ å¯¬é¬†ï¼ˆä¸æ¼èªï¼‰ï¼š{t_loose:.4f}")

    # å¯«æª”
    thresholds = {
        "conservative": float(t_conservative),
        "balanced": float(t_balanced),
        "loose": float(t_loose)
    }

    with open(os.path.join(MODEL_DIR, "threshold.json"), "w") as f:
        json.dump(thresholds, f, indent=4)

    print("\nğŸ’¾ å·²å¯«å…¥ threshold.json")
    return thresholds


# ================================
# å„²å­˜
# ================================
def save_all(X, y, knn, svm, centers, label_map, thresholds):
    np.save(f"{MODEL_DIR}/X.npy", X)
    np.save(f"{MODEL_DIR}/y.npy", y)

    with open(f"{MODEL_DIR}/knn.pkl", "wb") as f:
        pickle.dump(knn, f)

    with open(f"{MODEL_DIR}/svm.pkl", "wb") as f:
        pickle.dump(svm, f)

    with open(f"{MODEL_DIR}/centers.pkl", "wb") as f:
        pickle.dump(centers, f)

    with open(f"{MODEL_DIR}/label_map.json", "w") as f:
        json.dump(label_map, f, ensure_ascii=False, indent=2)

    print("\nğŸ’¾ æ‰€æœ‰æ¨¡å‹/è³‡æ–™å·²ä¿å­˜å®Œç•¢ï¼")


# ================================
# Main
# ================================
if __name__ == "__main__":
    X_old, y_old, label_map_old = load_old_data()

    # åµæ¸¬æ–°å¢äººç‰©
    existing_names = set(y_old.tolist())
    new_list = detect_new_persons(existing_names)

    if len(new_list) == 0 and len(X_old) > 0:
        print("\nâœ” æ²’æœ‰æ–°å¢äººå“¡ï¼Œä¸éœ€è¦é‡æ–°è¨“ç·´")
        exit()

    # æå– embedding
    X_new, y_new = extract_embeddings(new_list)

    # åˆä½µ
    X_raw = np.concatenate([X_old, X_new]) if len(X_old) > 0 else X_new
    y_raw = np.concatenate([y_old, y_new]) if len(y_old) > 0 else y_new

    # é‡å»º label_mapï¼ˆä¸­æ–‡ â†’ indexï¼‰
    unique_names = sorted(set(y_raw.tolist()))
    label_map = {name: idx for idx, name in enumerate(unique_names)}
    y_index = np.array([label_map[name] for name in y_raw])

    # é‡æ–°è¨“ç·´åˆ†é¡å™¨
    knn = train_knn(X_raw, y_raw)
    svm = train_svm(X_raw, y_raw)
    centers = calc_centers(X_raw, y_raw)

    # è‡ªå‹• threshold
    thresholds = auto_threshold_distance(X_raw, y_raw)

    # å„²å­˜
    save_all(X_raw, y_index, knn, svm, centers, label_map, thresholds)

    print("\nğŸ‰ ä¸‰åˆ†é¡å™¨è¨“ç·´å®Œæˆï¼")
