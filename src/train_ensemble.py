import os
import json
import pickle
import numpy as np
import cv2
from tqdm import tqdm
from insightface.app import FaceAnalysis

# ======================================================
# åŸºæœ¬è¨­å®š
# ======================================================
DATA_DIR = "/content/drive/MyDrive/face_DataSet"
RAW_DIR = f"{DATA_DIR}/face_raw"
MODEL_DIR = f"{DATA_DIR}/models"

os.makedirs(MODEL_DIR, exist_ok=True)

def imread_safe(path):
    return cv2.imdecode(np.fromfile(path, dtype=np.uint8), -1)

# ======================================================
# è¼‰å…¥èˆŠè³‡æ–™
# ======================================================
def load_old_embeddings():
    X_path = f"{MODEL_DIR}/X.npy"
    y_path = f"{MODEL_DIR}/y.npy"
    map_path = f"{MODEL_DIR}/label_map.json"

    if not (os.path.exists(X_path) and os.path.exists(y_path) and os.path.exists(map_path)):
        print("ğŸ“‚ ç¬¬ä¸€æ¬¡è¨“ç·´ï¼šæ²’æœ‰èˆŠè³‡æ–™")
        return np.array([]), np.array([]), {}

    print("ğŸ“‚ è¼‰å…¥èˆŠçš„ X.npy / y.npy / label_map.json")
    X = np.load(X_path)
    y = np.load(y_path)     # y å„²å­˜ indexï¼ˆä»¥ label_map å°æ‡‰ï¼‰
    label_map = json.load(open(map_path, "r", encoding="utf-8"))

    return X, y, label_map

# ======================================================
# æ‰¾å‡ºæ‰€æœ‰è®Šå‹•çš„äºº
# ======================================================
def detect_changes(label_map_old):
    current_people = sorted(os.listdir(RAW_DIR))
    old_people = sorted(list(label_map_old.keys()))

    added = list(set(current_people) - set(old_people))
    deleted = list(set(old_people) - set(current_people))
    same_people = list(set(current_people) & set(old_people))

    # æ¯”è¼ƒå„è‡ªç…§ç‰‡æ•¸é‡
    changed = []
    for person in same_people:
        raw_count = len(os.listdir(f"{RAW_DIR}/{person}"))
        # è€ label_map è£¡æ˜¯ indexï¼Œæ‰¾ä¸åˆ°ç…§ç‰‡æ•¸ â†’ è¦–ç‚ºè®Šå‹•
        # æˆ‘å€‘å¾ image_count.json è¨˜éŒ„æ•¸é‡
        pass

    # ä½¿ç”¨ image_count.json è¿½è¹¤ç…§ç‰‡æ•¸è®ŠåŒ–
    count_file = f"{MODEL_DIR}/image_count.json"
    old_count = {}
    if os.path.exists(count_file):
        old_count = json.load(open(count_file, "r"))
    else:
        old_count = {}

    new_count = {}
    for person in current_people:
        new_count[person] = len(os.listdir(f"{RAW_DIR}/{person}"))
        if person not in old_count or old_count[person] != new_count[person]:
            changed.append(person)

    json.dump(new_count, open(count_file, "w"), indent=2, ensure_ascii=False)

    print(f"ğŸ†• æ–°å¢äººå“¡ï¼š{added}")
    print(f"âŒ åˆªé™¤äººå“¡ï¼š{deleted}")
    print(f"â™»ï¸ ç…§ç‰‡æ•¸è®Šå‹•ï¼š{changed}")

    return added, deleted, changed

# ======================================================
# æŠ½å– embeddingsï¼ˆé‡å°æŸä¸€å€‹äººï¼‰
# ======================================================
app = FaceAnalysis(name="buffalo_l")
app.prepare(ctx_id=0)

def extract_person_embeddings(person):
    p_dir = f"{RAW_DIR}/{person}"
    imgs = os.listdir(p_dir)

    X_new = []
    print(f"\nğŸ“¸ é‡æ–°æŠ½å– {person}ï¼ˆ{len(imgs)} å¼µï¼‰")

    for img in tqdm(imgs):
        path = f"{p_dir}/{img}"
        img = imread_safe(path)
        if img is None:
            continue

        faces = app.get(img)
        if len(faces) == 0:
            continue

        X_new.append(faces[0].normed_embedding)

    return np.array(X_new)

# ======================================================
# é‡æ–°çµ„åˆ X / y
# ======================================================
def rebuild_dataset(X_old, y_old, label_map_old, added, deleted, changed):
    """
    è¦å‰‡ Bï¼ˆä½ é¸çš„ï¼‰ï¼š
    è‹¥æŸäººç…§ç‰‡æœ‰è®Š â†’ é‡æŠ½è©²äººçš„ embeddings
    è‹¥æ–°å¢ â†’ æ–°å¢ embeddings
    è‹¥åˆªé™¤ â†’ ç§»é™¤æ‰€æœ‰èˆŠ embeddings
    """

    # ç•¶å‰ RAW è³‡æ–™å¤¾ä¸­äººç‰©åˆ—è¡¨
    current_people = sorted(os.listdir(RAW_DIR))

    # æ–°çš„ label_mapï¼ˆé‡æ–°æ’åºï¼‰
    new_label_map = {p: i for i, p in enumerate(current_people)}
    new_X = []
    new_y = []

    # --- å°æ¯å€‹ç¾å­˜çš„äººåšè™•ç† ---
    for person in current_people:
        if person in added or person in changed:
            # ğŸ”¥ å¿…é ˆé‡æŠ½
            Xp = extract_person_embeddings(person)
        else:
            # ğŸ”¥ å¾èˆŠè³‡æ–™æŒ‘å‡º
            if person in label_map_old:
                old_idx = label_map_old[person]
                Xp = X_old[y_old == old_idx]
            else:
                # ç†è«–ä¸Šä¸æœƒç™¼ç”Ÿ
                Xp = extract_person_embeddings(person)

        # åŠ å…¥åˆ°æ–°é›†åˆ
        label_idx = new_label_map[person]
        for emb in Xp:
            new_X.append(emb)
            new_y.append(label_idx)

    new_X = np.array(new_X)
    new_y = np.array(new_y)

    return new_X, new_y, new_label_map

# ======================================================
# è¨“ç·´ KNN / SVM / Centers
# ======================================================
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

def train_knn(X, y):
    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X, y)
    return knn

def train_svm(X, y):
    svm = SVC(kernel="linear", probability=True)
    svm.fit(X, y)
    return svm

def calc_centers(X, y):
    centers = {}
    for idx in np.unique(y):
        centers[idx] = X[y == idx].mean(axis=0)
    return centers

# ======================================================
# ä¿å­˜æ¨¡å‹
# ======================================================
def save_all(X, y, label_map, knn, svm, centers):
    np.save(f"{MODEL_DIR}/X.npy", X)
    np.save(f"{MODEL_DIR}/y.npy", y)

    json.dump(label_map, open(f"{MODEL_DIR}/label_map.json", "w", encoding="utf-8"), indent=2, ensure_ascii=False)
    pickle.dump(knn, open(f"{MODEL_DIR}/knn.pkl", "wb"))
    pickle.dump(svm, open(f"{MODEL_DIR}/svm.pkl", "wb"))
    pickle.dump(centers, open(f"{MODEL_DIR}/centers.pkl", "wb"))

    print("\nğŸ’¾ å·²ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œè³‡æ–™ï¼")

# ======================================================
# Main
# ======================================================
if __name__ == "__main__":

    X_old, y_old, label_map_old = load_old_embeddings()
    added, deleted, changed = detect_changes(label_map_old)

    # è‹¥æ²’æœ‰è®ŠåŒ– â†’ ä¸è¨“ç·´
    if len(added) + len(deleted) + len(changed) == 0:
        print("âœ” æ²’æœ‰è®ŠåŒ–ï¼Œä¸éœ€é‡æ–°è¨“ç·´")
        exit()

    print("\nğŸš€ é–‹å§‹å¢é‡è¨“ç·´ï¼ˆB ç‰ˆ / æœ€ä¹¾æ·¨æ¨¡å¼ï¼‰")

    X_new, y_new, new_label_map = rebuild_dataset(
        X_old, y_old, label_map_old,
        added, deleted, changed
    )

    print("\nğŸ”§ è¨“ç·´ KNN / SVM / Centers ä¸­...")
    knn = train_knn(X_new, y_new)
    svm = train_svm(X_new, y_new)
    centers = calc_centers(X_new, y_new)

    save_all(X_new, y_new, new_label_map, knn, svm, centers)

    print("\nğŸ‰ é‡æ–°è¨“ç·´å®Œæˆï¼")
